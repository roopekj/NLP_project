{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentence BERT clustering demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetch news dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "newsgroups_train = fetch_20newsgroups(subset='train', remove=('headers', 'footers', 'quotes'))\n",
    "idxs = [ i for i in range(len(newsgroups_train.data)) if len(newsgroups_train.data[i]) > 20 ]\n",
    "newsgroups_train.data = [ newsgroups_train.data[i] for i in idxs ]\n",
    "newsgroups_train.target = [ newsgroups_train.target[i] for i in idxs ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data size:  10955\n",
      "Average train data length:  191.90652670013694\n",
      "Train data labels:  ['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med', 'sci.space', 'soc.religion.christian', 'talk.politics.guns', 'talk.politics.mideast', 'talk.politics.misc', 'talk.religion.misc']\n",
      "Number of train data labels:  20\n",
      "Example:\n",
      "[rec.autos]  I was wondering if anyone out there could enlighten me on this car I saw\n",
      "the other day. It was a 2-door sports car, looked to be from the late 60s/\n",
      "early 70s. It was called a Bricklin. The doors were really small. In addition,\n",
      "the front bumper was separate from the rest of the body. This is \n",
      "all I know. If anyone can tellme a model name, engine specs, years\n",
      "of production, where this car is made, history, or whatever info you\n",
      "have on this funky looking car, please e-mail.\n"
     ]
    }
   ],
   "source": [
    "print(\"Train data size: \", len(newsgroups_train.data))\n",
    "\n",
    "print(\"Average train data length: \", np.mean([len(x.split()) for x in newsgroups_train.data]))\n",
    "\n",
    "print(\"Train data labels: \", newsgroups_train.target_names)\n",
    "print(\"Number of train data labels: \", len(newsgroups_train.target_names))\n",
    "\n",
    "print(f\"Example:\\n[{newsgroups_train.target_names[newsgroups_train.target[0]]}] \", newsgroups_train.data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('app/data/samples.json', 'w') as f:\n",
    "    json.dump(newsgroups_train.data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 343/343 [02:31<00:00,  2.26it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(10955, 384)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "train_embeddings = model.encode(newsgroups_train.data, show_progress_bar=True)\n",
    "train_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ser/.envs/aalto-nlp/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "num_clusters = 20 # Could be optimized later using validation indices\n",
    "kmeans = KMeans(n_clusters=num_clusters, random_state=0).fit(train_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('app/data/labels.json', 'w') as f:\n",
    "    json.dump(kmeans.labels_.tolist(), f)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the graph using tSNE\n",
    "We use tSNE simply to reduce the dimensionality of the embeddings to 2D for visualization. This is unnecessary; should remove."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "tsne = TSNE(n_components=2, n_iter=1000, n_iter_without_progress=200, perplexity=35)\n",
    "tsne_embeddings = tsne.fit_transform(train_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 9))\n",
    "plt.scatter(tsne_embeddings[:, 0], tsne_embeddings[:, 1], c=kmeans.labels_, cmap='tab20')\n",
    "plt.legend(handles=[plt.scatter([],[],color=plt.cm.tab20(i/20), label=f\"Cluster {i}\") for i in range(20)])\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(16, 9))\n",
    "plt.scatter(tsne_embeddings[:, 0], tsne_embeddings[:, 1], c=newsgroups_train.target, cmap='tab20')\n",
    "plt.legend(handles=[plt.scatter([],[],color=plt.cm.tab20(i/20), label=newsgroups_train.target_names[i]) for i in range(20)])\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the graph using PyVis\n",
    "Testing the library Pyvis to create an interactive graph in 2D. Good luck loading this lol."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyvis.network import Network\n",
    "net_pred = Network(notebook=True, cdn_resources=\"in_line\")\n",
    "net_trgt = Network(notebook=True)\n",
    "\n",
    "for i in range(len(train_embeddings)):\n",
    "    net_pred.add_node(i, label=newsgroups_train.data[i][:50], color=plt.cm.tab20(kmeans.labels_[i]/20))\n",
    "    net_trgt.add_node(i, label=newsgroups_train.data[i][:50], color=plt.cm.tab20(newsgroups_train.target[i]/20))\n",
    "\n",
    "net_pred.show(\"pred.html\")\n",
    "net_trgt.show(\"trgt.html\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cluster labelling\n",
    "We use LDA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate topics using LDA for each cluster\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "labels = {i : '' for i in range(num_clusters)}\n",
    "\n",
    "cluster_keywords = []\n",
    "\n",
    "for i in range(num_clusters):\n",
    "    cluster_data = [newsgroups_train.data[j] for j in range(len(newsgroups_train.data)) if kmeans.labels_[j] == i]\n",
    "    vectorizer = CountVectorizer(max_df=0.95, min_df=2, max_features=1000, stop_words='english')\n",
    "    X = vectorizer.fit_transform(cluster_data)\n",
    "    lda = LatentDirichletAllocation(n_components=5, max_iter=5, learning_method='online', learning_offset=50.,random_state=0).fit(X)\n",
    "    labels[i] = ', '.join([vectorizer.get_feature_names_out()[i] for i in [\n",
    "        idx for idx in lda.components_[0].argsort() if not vectorizer.get_feature_names_out()[idx].isdigit()\n",
    "    ][:-1 - 1:-1]])\n",
    "    print(f\"Cluster {i}\")\n",
    "    print(\"Top 10 words per topic:\")\n",
    "    kwds = []\n",
    "    for topic_idx, topic in enumerate(lda.components_):\n",
    "        feature_names = [vectorizer.get_feature_names_out()[i] for i in topic.argsort()[:-10 - 1:-1] if vectorizer.get_feature_names_out()[i] not in kwds and not vectorizer.get_feature_names_out()[i].isdigit()]\n",
    "        kwds.extend(feature_names)\n",
    "        print(f\"Topic {topic_idx}: {', '.join(feature_names)}\")\n",
    "    cluster_keywords.append(kwds)\n",
    "    print()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtaining a 1-word summary of the keywords\n",
    "It does not work well for now, we could perhaps improve it by training a model on (doc, topic) pairs comming from the intial dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, GPT2LMHeadModel\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('gpt2')\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "\n",
    "query = f\"A single word to define the topic given by the keywords [{', '.join(cluster_keywords[2])}] is: \"\n",
    "inputs = tokenizer(query, return_tensors='pt').input_ids\n",
    "outputs = model.generate(inputs, max_new_tokens=10, do_sample=False)\n",
    "label = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 9))\n",
    "plt.scatter(tsne_embeddings[:, 0], tsne_embeddings[:, 1], c=kmeans.labels_, cmap='tab20')\n",
    "plt.legend(handles=[plt.scatter([],[],color=plt.cm.tab20(i/20), label=f\"Cluster {i}: {labels[i]}\") for i in range(20)])\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(16, 9))\n",
    "plt.scatter(tsne_embeddings[:, 0], tsne_embeddings[:, 1], c=newsgroups_train.target, cmap='tab20')\n",
    "plt.legend(handles=[plt.scatter([],[],color=plt.cm.tab20(i/20), label=newsgroups_train.target_names[i]) for i in range(20)])\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
